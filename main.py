import Hand_crafted_features as HCF
from lab2_dataset import SatelliteSet
import torch
import h5py
import numpy as np
from tqdm import tqdm
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import SGDRegressor
import pickle
from datetime import datetime
import os
from sklearn.metrics import mean_squared_error, mean_absolute_error

def getFeatures(x):
    R = np.expand_dims(x[:, :, 0], axis=-1)
    G = np.expand_dims(x[:, :, 1], axis=-1)
    B = np.expand_dims(x[:, :, 2], axis=-1)
    NIR = np.expand_dims(x[:, :, 3], axis=-1)
    NDVI = np.expand_dims(HCF.getNDVI(x), axis=-1)
    MSAVI = np.expand_dims(HCF.getMSAVI(x), axis=-1)
    VARI = np.expand_dims(HCF.getVARI(x), axis=-1)
    ARVI = np.expand_dims(HCF.getARVI(x), axis=-1)
    GCI = np.expand_dims(HCF.getGCI(x), axis=-1)
    SIPI = np.expand_dims(HCF.getSIPI(x), axis=-1)
    HSV = HCF.HSV(x)
    LAB = HCF.LAB(x)
    # SOBEL = HCF.SOBEL(x)
    # PREWITT = HCF.PREWITT(x)
    # LBP = HCF.LBP(x)

    # hcf_list = [R, G, B, NIR, NDVI, MSAVI, VARI, ARVI, GCI, SIPI,
    #             HSV, LAB, SOBEL, PREWITT, LBP]
    # hcf_list = [R,G,B,NIR,NDVI]
    hcf_list = [R,G,B,NIR,NDVI,MSAVI, VARI, ARVI, GCI, SIPI, HSV, LAB]
    all_features = np.concatenate(hcf_list, axis=2)
    return all_features

if __name__ == "__main__":
    # define the dataset and dataloader
    dset = SatelliteSet(windowsize=256, split='train')
    train_loader = torch.utils.data.DataLoader(dset,
                                            batch_size=8,
                                            num_workers=0,
                                            shuffle=False)
    validate_set = SatelliteSet(windowsize=256, split='validate')
    validate_loader = torch.utils.data.DataLoader(validate_set,
                                            batch_size=1,
                                            num_workers=0,
                                            shuffle=False)

    # define the regressors
    learning_rate = 'constant'
    eta0 = 0.01
    reg = SGDRegressor(max_iter=1000, tol=1e-3, learning_rate=learning_rate, eta0=eta0)
    scaler_x = StandardScaler()
    scaler_y = StandardScaler()

    count = 0
    for X, Y in tqdm(train_loader):
        # iterate in one small batch
        X = np.transpose(X, (0, 2, 3, 1))

        for i in range(X.shape[0]):
            x = np.asarray(X[i])
            y = Y[i]
            # print(x.shape,y.shape)
            all_features = getFeatures(x)

            # first to standardize features and then start the regression
            all_feat_2D = np.reshape(all_features, (-1, all_features.shape[-1]))  # reshape into a 2D matrix (n*feature numbers) for scaler and regression
            all_feat_2D[np.isinf(all_feat_2D)] = 0
            all_feat_2D[np.isnan(all_feat_2D)] = 0 # replace nan and inf (generated by 0/0) with 0
            
            y_2D = np.reshape(y, (-1, 1)).numpy().astype(np.float32)
            y_2D[np.isnan(y_2D)] = 0
            y_2D[np.isinf(y_2D)] = 0

            scaler_x.partial_fit(all_feat_2D)
            scaler_y.partial_fit(y_2D)
            std_x = scaler_x.transform(all_feat_2D)
            std_y = scaler_y.transform(y_2D).flatten()
            reg.partial_fit(std_x, std_y)

        count = count + 1
        # save trained model every 50 baches
        if count%20 == 0:
            #evaluate on validate set
            print("\nEvaluate the performance on validate set")
            true_label_list = []
            pred_label_list = []
            iter_count = 0
            for X1,Y1 in validate_set:
                if iter_count == 100:
                    break
                X1 = np.transpose(X1,(1,2,0))
                all_feat = getFeatures(np.asarray(X1))
                all_feat_2D = np.reshape(all_feat, (-1, all_feat.shape[-1]))
                all_feat_2D[np.isinf(all_feat_2D)] = 0
                all_feat_2D[np.isnan(all_feat_2D)] = 0 # replace nan and inf (generated by 0/0) with 0
                # print(all_feat.shape)

                

                y_2D = np.reshape(Y1, (-1, 1)).astype(np.float32)
                y_2D[np.isnan(y_2D)] = 0
                y_2D[np.isinf(y_2D)] = 0
                
                std_x = scaler_x.transform(all_feat_2D)
                
                # print(std_x.shape,std_y.shape)
                predict = reg.predict(std_x)
                # print(predict.shape)
                predict = np.reshape(predict,(-1,1))
                # print(y_2D.shape,predict.shape)
                y_predict = scaler_y.inverse_transform(predict)
                true_label_list.append(y_2D)
                pred_label_list.append(y_predict)
                iter_count += 1
            y_true = np.concatenate(true_label_list)
            y_pred = np.concatenate(pred_label_list)
            mse = mean_absolute_error(y_true, y_pred)
            print('MAE:',mse)
                

                
            folder = '..\checkpoint' + os.sep + 'SGD_lr_' + str(eta0) +'_'+str(mse)
            if not os.path.exists(folder):
                os.makedirs(folder)
            model_time = datetime.now().strftime('%m%d-%H-%M-%S')
            pkl_filename = folder + os.sep + 'SGD_' + model_time + '.pkl'
            with open(pkl_filename, 'wb') as file:
                pickle.dump(reg, file)


    print("Done fitting")


    # load trained model from file
    # pkl_filename = ''
    # with open(pkl_filename, 'rb') as file:
    #     trained_reg = pickle.load(file)


    # start predicting
    # reg_pre = reg.predict()
    h5 = h5py.File('..\data\dataset_rgb_nir_test.hdf5', 'r')


